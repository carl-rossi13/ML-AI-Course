{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Problem\n",
    "\n",
    "Create a supervised machine learning model to predict the price of a used car based on its attributes, such as mileage, age, drivetrain, engine type, and brand.\n",
    "\n",
    "Use statistical and machine learning techniques to determine the relative importance of each feature in influencing the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms\n",
    "\n",
    "Technical Vocabulary:\n",
    "\n",
    "Target Variable: The sale price of the car (dependent variable).\n",
    "Features: Car attributes such as mileage, age, cylinder count, fuel type, transmission, drivetrain, brand, and condition.\n",
    "Task Type: Regression (predicting a continuous variable) combined with feature importance analysis to interpret the model.\n",
    "Modeling Goals:\n",
    "Develop a regression model that minimizes error (e.g., Mean Squared Error).\n",
    "Identify features with the highest coefficients or feature importance scores to provide actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import psutil\n",
    "\n",
    "# Set configuration for pipeline diagram display\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/vehicles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataframe to verify\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the dataset before any processing\n",
    "print(\"Initial size of the dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values and their frequencies in the 'cylinders' column:\n",
    "# print(df['cylinders'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values and their frequencies in the 'fuel' column:\n",
    "# print(df['fuel'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values and their frequencies in the 'transmission' column:\n",
    "# print(df['transmission'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values and their frequencies in the 'drive' column:\n",
    "# print(df['drive'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values and their frequencies in the 'size' column:\n",
    "# print(df['size'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values and their frequencies in the 'type' column:\n",
    "# print(df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values and their frequencies in the 'manufacturer' column:\n",
    "# print(df['manufacturer'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values and their frequencies in the 'state' column:\n",
    "# print(df['state'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "print(\"Size of the dataset after removing duplicates:\", df.shape)\n",
    "\n",
    "# Remove rows where 'price' is 0, negative, blank, or above 200,000\n",
    "df = df[(df['price'] > 0) & (df['price'] <= 200000)]  \n",
    "print(f\"Dataset shape after filtering: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New Features\n",
    "\n",
    "# Custom Feature 1 - Age of the Car\n",
    "current_year = pd.Timestamp.now().year\n",
    "df['car_age'] = current_year - df['year']\n",
    "\n",
    "# Custom Feature 2 - Luxury Indicator\n",
    "luxury_brands = [\n",
    "    'bmw', \n",
    "    'mercedes-benz', \n",
    "    'lexus', \n",
    "    'audi', \n",
    "    'cadillac', \n",
    "    'acura', \n",
    "    'infiniti', \n",
    "    'lincoln', \n",
    "    'volvo', \n",
    "    'porsche', \n",
    "    'jaguar', \n",
    "    'land rover', \n",
    "    'alfa-romeo', \n",
    "    'ferrari', \n",
    "    'aston-martin'\n",
    "]\n",
    "\n",
    "df['is_luxury'] = df['manufacturer'].apply(lambda x: 1 if x in luxury_brands else 0)\n",
    "\n",
    "# 4. Vehicle Size and Type\n",
    "#df['size'] = df['size'].fillna('unknown')  # Handle missing values\n",
    "#df['type'] = df['type'].fillna('unknown')\n",
    "\n",
    "# Custom Feature 3 - Condition Index\n",
    "condition_mapping = {\n",
    "    'new': 5,\n",
    "    'like new': 4,\n",
    "    'excellent': 3,\n",
    "    'good': 2,\n",
    "    'fair': 1,\n",
    "    'salvage': 0\n",
    "}\n",
    "df['condition_index'] = df['condition'].map(condition_mapping)\n",
    "\n",
    "# Custom Feature 4 - eco (hybrid or electric)\n",
    "df['eco'] = df['fuel'].apply(lambda x: 1 if x in ['hybrid', 'electric'] else 0)\n",
    "\n",
    "# Custom Feature 5 - Paint Popularity\n",
    "popular_colors = ['black', 'white', 'silver', 'gray', 'blue']\n",
    "df['is_popular_color'] = df['paint_color'].apply(lambda x: 1 if x in popular_colors else 0)\n",
    "\n",
    "# Custom Feature 6 - Drive Code\n",
    "df['drive_code'] = df['drive'].apply(lambda x: 1 if x == '4wd' else 0)\n",
    "\n",
    "# Custom Feature 7 - Title Code\n",
    "df['title_code'] = df['title_status'].apply(lambda x: 1 if x == 'clean' else 0)\n",
    "\n",
    "# Custom Feature 8 - Transmission Code\n",
    "df['transmission_code'] = df['transmission'].apply(lambda x: 1 if x == 'automatic' else 0)\n",
    "\n",
    "# Custom Feature 9 - Cylinder Code\n",
    "df = df[df['cylinders'].notna() & (df['cylinders'] != 'other')]\n",
    "\n",
    "# Extract the numerical part from the 'cylinders' column and create a new column 'cylinder_code'\n",
    "df['cylinder_code'] = df['cylinders'].str.extract(r'(\\d+)')  # Use raw string for regex\n",
    "\n",
    "# Drop rows where extraction failed (results in NaN) and convert to integer\n",
    "df = df[df['cylinder_code'].notna()]\n",
    "df['cylinder_code'] = df['cylinder_code'].astype(int)\n",
    "\n",
    "# Custom Feature 10 - Group States by Geographic Region\n",
    "state_to_region = {\n",
    "    'AL': 'South', 'AK': 'West', 'AZ': 'West', 'AR': 'South', 'CA': 'West',\n",
    "    'CO': 'West', 'CT': 'Northeast', 'DE': 'South', 'FL': 'South', 'GA': 'South',\n",
    "    'HI': 'West', 'ID': 'West', 'IL': 'Midwest', 'IN': 'Midwest', 'IA': 'Midwest',\n",
    "    'KS': 'Midwest', 'KY': 'South', 'LA': 'South', 'ME': 'Northeast', 'MD': 'South',\n",
    "    'MA': 'Northeast', 'MI': 'Midwest', 'MN': 'Midwest', 'MS': 'South', 'MO': 'Midwest',\n",
    "    'MT': 'West', 'NE': 'Midwest', 'NV': 'West', 'NH': 'Northeast', 'NJ': 'Northeast',\n",
    "    'NM': 'West', 'NY': 'Northeast', 'NC': 'South', 'ND': 'Midwest', 'OH': 'Midwest',\n",
    "    'OK': 'South', 'OR': 'West', 'PA': 'Northeast', 'RI': 'Northeast', 'SC': 'South',\n",
    "    'SD': 'Midwest', 'TN': 'South', 'TX': 'South', 'UT': 'West', 'VT': 'Northeast',\n",
    "    'VA': 'South', 'WA': 'West', 'WV': 'South', 'WI': 'Midwest', 'WY': 'West',\n",
    "    'DC': 'South'  # Adding DC as part of the South region\n",
    "}\n",
    "\n",
    "# Convert all entries in the 'state' column to uppercase\n",
    "df['state'] = df['state'].str.upper()\n",
    "\n",
    "# Reapply the mapping after converting to uppercase\n",
    "df['state_region'] = df['state'].map(state_to_region)\n",
    "\n",
    "# Assign a unique code to each region\n",
    "region_mapping = {region: idx + 1 for idx, region in enumerate(df['state_region'].unique())}\n",
    "\n",
    "# Map the region codes to a new column\n",
    "df['region_code'] = df['state_region'].map(region_mapping)\n",
    "\n",
    "# Handling Missing Values\n",
    "df['car_age'] = df['car_age'].fillna(df['car_age'].median())\n",
    "df['condition_index'] = df['condition_index'].fillna(0)  \n",
    "df['drive'] = df['drive'].fillna('unknown')\n",
    "df['transmission'] = df['transmission'].fillna('unknown')\n",
    "df['fuel'] = df['fuel'].fillna('unknown') \n",
    "\n",
    "# Drop rows with odometer > 500000 or car_age > 100\n",
    "df = df[(df['odometer'] <= 500000) & (df['car_age'] <= 100)]\n",
    "df.drop(columns=['id', 'VIN', 'paint_color', 'region', 'condition', 'model'], inplace=True)\n",
    "\n",
    "# Check the new dataset shape\n",
    "print(f\"Dataset shape after filtering: {df.shape}\")\n",
    "\n",
    "print(\"Data preparation complete. Ready for preprocessing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total and available memory\n",
    "total_memory = psutil.virtual_memory().total / (1024 ** 3)  # Convert to GB\n",
    "available_memory = psutil.virtual_memory().available / (1024 ** 3)  # Convert to GB\n",
    "\n",
    "print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "print(f\"Available Memory: {available_memory:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = ['cylinder_code', 'eco', 'odometer', 'title_code', \n",
    "            'region_code', 'car_age', 'is_luxury', \n",
    "            'condition_index', 'is_popular_color', 'transmission_code', 'drive_code']\n",
    "\n",
    "X = df[features].copy()\n",
    "\n",
    "# Log-transform the target\n",
    "df['price_log'] = np.log1p(df['price'])\n",
    "\n",
    "y = df['price_log']\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_columns = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Impute missing values\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "X[numerical_columns] = num_imputer.fit_transform(X[numerical_columns])\n",
    "\n",
    "# One-hot encode categorical features\n",
    "# X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the target variable\n",
    "#y_scaler = StandardScaler()\n",
    "#y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "#y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "model_selector_pipe = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=3, include_bias=False)),  # Adjust degree if necessary\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', SelectFromModel(Lasso(alpha=0.05, max_iter=3000))),  # Increased alpha and max_iter\n",
    "    ('linreg', LinearRegression())\n",
    "])\n",
    "\n",
    "model_selector_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "selector_train_mse = mean_squared_error(y_train, model_selector_pipe.predict(X_train))\n",
    "selector_test_mse = mean_squared_error(y_test, model_selector_pipe.predict(X_test))\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "#predictions_original_scale = y_scaler.inverse_transform(model_selector_pipe.predict(X_test).reshape(-1, 1)).ravel()\n",
    "\n",
    "# Results\n",
    "print(\"Train MSE:\", selector_train_mse)\n",
    "print(\"Test MSE:\", selector_test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSE on the log-transformed (scaled) target\n",
    "lasso_train_mse_scaled = mean_squared_error(y_train, model_selector_pipe.predict(X_train))\n",
    "lasso_test_mse_scaled = mean_squared_error(y_test, model_selector_pipe.predict(X_test))\n",
    "\n",
    "# Convert predictions back to the original scale (reverse log transformation)\n",
    "train_predictions_original = np.expm1(model_selector_pipe.predict(X_train))\n",
    "test_predictions_original = np.expm1(model_selector_pipe.predict(X_test))\n",
    "\n",
    "# Convert actual values back to the original scale (reverse log transformation)\n",
    "y_train_original = np.expm1(y_train)\n",
    "y_test_original = np.expm1(y_test)\n",
    "\n",
    "# Compute MSE on the original scale\n",
    "lasso_train_mse = mean_squared_error(y_train_original, train_predictions_original)\n",
    "lasso_test_mse = mean_squared_error(y_test_original, test_predictions_original)\n",
    "\n",
    "# Results\n",
    "print(\"Train MSE (Log-Transformed Scale):\", lasso_train_mse_scaled)\n",
    "print(\"Test MSE (Log-Transformed Scale):\", lasso_test_mse_scaled)\n",
    "print(\"Train MSE (Original Scale):\", lasso_train_mse)\n",
    "print(\"Test MSE (Original Scale):\", lasso_test_mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names after polynomial feature expansion\n",
    "feature_names = model_selector_pipe.named_steps['poly_features'].get_feature_names_out(X_train.columns)\n",
    "\n",
    "# Extract coefficients from the Lasso step\n",
    "lasso_coefs = model_selector_pipe.named_steps['selector'].estimator_.coef_\n",
    "\n",
    "# Create a DataFrame to display features and their corresponding coefficients\n",
    "lasso_df = pd.DataFrame({'feature': feature_names, 'coef': lasso_coefs})\n",
    "\n",
    "# Sort the DataFrame by the absolute value of coefficients for better insight\n",
    "lasso_df['abs_coef'] = lasso_df['coef'].abs()\n",
    "lasso_df = lasso_df.sort_values(by='abs_coef', ascending=False)\n",
    "\n",
    "# Results\n",
    "print(type(feature_names))\n",
    "print(lasso_df.loc[lasso_df['coef'] != 0])  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Top 20 Features by Coefficient Value (Log-Transformed Target)\n",
    "sorted_lasso_df = lasso_df.head(20) \n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(\n",
    "    sorted_lasso_df['feature'],\n",
    "    sorted_lasso_df['coef'],\n",
    "    color=['red' if c < 0 else 'blue' for c in sorted_lasso_df['coef']]\n",
    ")\n",
    "plt.xlabel('Coefficient Value (Log-Transformed Target)', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 20 Factors by Coefficient Value', fontsize=14)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on log-transformed scale\n",
    "y_train_pred_log = model_selector_pipe.predict(X_train)\n",
    "y_test_pred_log = model_selector_pipe.predict(X_test)\n",
    "\n",
    "# Convert predictions back to the original scale\n",
    "y_train_pred_original = np.expm1(y_train_pred_log)\n",
    "y_test_pred_original = np.expm1(y_test_pred_log)\n",
    "\n",
    "# Convert actual values back to the original scale\n",
    "y_train_original = np.expm1(y_train)\n",
    "y_test_original = np.expm1(y_test)\n",
    "\n",
    "# Residuals on the original scale\n",
    "residuals_test = y_test_original - y_test_pred_original\n",
    "\n",
    "# Residual plot for test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_original, residuals_test, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residual Plot (Original Scale)', fontsize=14)\n",
    "plt.xlabel('Actual Price', fontsize=12)\n",
    "plt.ylabel('Residuals', fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pd.DataFrame(lasso_df.set_index('feature')['coef']).T, cmap='coolwarm', annot=True, fmt=\".2f\", cbar=True)\n",
    "plt.title('Feature Coefficients Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features = ['odometer', 'car_age', 'price']\n",
    "sns.pairplot(df[key_features])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings and Suggestions:\n",
    "1.\tCars with larger engines (higher cylinder counts i.e. ‘cylinder_code’) significantly increase the sale price. \n",
    "•\tSuggestion: Include vehicles with 6, 8, or more cylinders in your inventory, as they are highly valued by buyers, especially for performance or utility purposes.\n",
    "2.\tHigher odometer readings (mileage) combined with the car’s age have a strong negative impact on price (‘odometer car_age’). Higher mileage is consistently associated with a decrease in price (‘odometer’).\n",
    "•\tSuggestion: Prioritize acquiring cars with low mileage, especially for older models, to maintain higher resale values.\n",
    "3.\tOlder cars see a notable decline in value, even without factoring in mileage or other features (‘car_age’).\n",
    "•\tSuggestion: Focus on stocking newer models whenever possible. Highlight the condition and features of older vehicles to mitigate price reductions.\n",
    "4.\tCertain combinations of transmission type and car age positively influence price (‘car_age transmission_code’). Vehicles with popular colors, specific transmission types, and drivetrain configurations tend to attract higher prices (is_popular_color transmission_code drive_code’).\n",
    "•\tSuggestion: Acquire cars with automatic transmissions, especially those paired with desirable colors or AWD/4WD configurations.\n",
    "5.\tPopular exterior colors marginally increase sale price (‘is_popular_color’). Luxury vehicles or configurations positively influence pricing (‘is_luxury’)\n",
    "•\tSuggestion: Stock luxury brands and highlight popular colors to attract buyers willing to pay a premium.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
